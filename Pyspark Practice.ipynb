{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e3413-7e23-4b84-af44-598ede96bc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce7acca-1b79-4051-bf29-20c4907fbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5674995-3ee2-465b-9b65-1b125b074c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55428c6-37cb-4429-81a5-e585ebc48e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73926da4-4443-452f-9ed3-569c17c496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Rudra\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c7b82f6-bb6e-49b2-81f9-77bf1d74425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf24d71-04c7-4c7b-9c7f-8f0ea8dc5ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "# counts = words.count()\n",
    "# words.collect()\n",
    "coll = words.collect()\n",
    "print(\"Elements in RDD -> %s\" % (coll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0127dc1-6b64-4a68-9d8b-eec91003d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"foreach(f)\n",
    "Returns only those elements which meet the condition of the function inside foreach. In the following example, we call a print function in foreach, which prints all the elements in the RDD.\"\n",
    "\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "def f(x):print(x)\n",
    "fore = words.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc1116d8-31d0-4fcf-a841-663fc38b64df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"filter(f)\n",
    "A new RDD is returned containing the elements, which satisfies the function inside the filter.\n",
    "In the following example, we filter out the strings containing ''spark\".\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Fitered RDD -> %s\" % (filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e58bb259-7ada-4dbe-83f4-0b852e1c8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair -> [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"map(f, preservesPartitioning = False)\n",
    "A new RDD is returned by applying a function to each element in the RDD. \n",
    "In the following example, we form a key value pair and map every string with a value of 1.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(\"Key value pair -> %s\" % (mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d70160bf-4967-4b1c-8547-2cd659713eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"reduce(f)\n",
    "After performing the specified commutative and associative binary operation, the element in the RDD is returned. \n",
    "In the following example,\n",
    "we are importing add package from the operator and applying it on ‘num’ to carry out a simple addition operation.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print(\"Adding all the elements -> %i\" % (adding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d19b913b-7d69-429f-959a-332b945e6f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"join(other, numPartitions = None)\n",
    "It returns RDD with a pair of elements with the matching keys and all the values for that particular key. \n",
    "In the following example, there are two pair of elements in two different RDDs.\n",
    "After joining these two RDDs, we get an RDD with elements having matching keys and their values.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print(\"Join RDD -> %s\" % (final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd05c98-d7fa-40e6-b843-630c2c9d785a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"cache()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mPersist this RDD with the default storage level (MEMORY_ONLY). \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mYou can also check if the RDD is cached or not.\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m----> 5\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mparallelize (\n\u001b[0;32m      6\u001b[0m    [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscala\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      7\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      8\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhadoop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      9\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     10\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124makka\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark vs hadoop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     12\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyspark\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyspark and spark\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m ) \n\u001b[0;32m     15\u001b[0m words\u001b[38;5;241m.\u001b[39mcache() \n\u001b[0;32m     16\u001b[0m caching \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39mpersist()\u001b[38;5;241m.\u001b[39mis_cached \n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"cache()\n",
    "Persist this RDD with the default storage level (MEMORY_ONLY). \n",
    "You can also check if the RDD is cached or not.\"\"\"\n",
    "from pyspark import SparkContext\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ") \n",
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print(\"Words got chached > %s\" % (caching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5eb092-71b2-4521-80cf-8cad36e2ae0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
